{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "02-CNN-Project-Exercise.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba7xqNxbJJLi",
        "colab_type": "text"
      },
      "source": [
        "## CNN-Project-Exercise\n",
        "We'll be using the CIFAR-10 dataset, which is very famous dataset for image recognition! \n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34qnjKcTJJLk",
        "colab_type": "text"
      },
      "source": [
        "** Download the data for CIFAR from here: https://www.cs.toronto.edu/~kriz/cifar.html **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDY9aXOJJLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CIFAR_DIR = 'cifar-10-batches-py/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apJZOwkyJJLr",
        "colab_type": "text"
      },
      "source": [
        "The archive contains the files data_batch_1, data_batch_2, ..., data_batch_5, as well as test_batch. Each of these files is a Python \"pickled\" object produced with cPickle. \n",
        "\n",
        "** Load the Data. Use the Code Below to load the data: **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfp6fX46JJLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
        "    return cifar_dict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ln3ePGPJJLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNLF46ZPJJL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = [0,1,2,3,4,5,6]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJnI__-sJJL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,direc in zip(all_data,dirs):\n",
        "    all_data[i] = unpickle(CIFAR_DIR+direc)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp5b03ypJJL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_meta = all_data[0]\n",
        "data_batch1 = all_data[1]\n",
        "data_batch2 = all_data[2]\n",
        "data_batch3 = all_data[3]\n",
        "data_batch4 = all_data[4]\n",
        "data_batch5 = all_data[5]\n",
        "test_batch = all_data[6]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM2kLmdiJJML",
        "colab_type": "text"
      },
      "source": [
        "Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
        "* data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
        "* labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
        "\n",
        "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
        "\n",
        "* label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ1sZS0TJJMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXYSkihMJJMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_batch1[b\"data\"] \n",
        "X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3wDV6cIJJMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "5799f8bf-243e-4c6a-9791-e781a8f91fa1"
      },
      "source": [
        "plt.imshow(X[6])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc136f83390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6WmKhXQJJMh",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions for Dealing With Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbVMYpgIJJMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(vec, vals=10):\n",
        "    '''\n",
        "    For use to one-hot encode the 10- possible labels\n",
        "    '''\n",
        "    n = len(vec)\n",
        "    out = np.zeros((n, vals))\n",
        "    out[range(n), vec] = 1\n",
        "    return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpHDWM2QJJMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CifarHelper():\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.i = 0\n",
        "        \n",
        "        # Grabs a list of all the data batches for training\n",
        "        self.all_train_batches = [data_batch1,data_batch2,data_batch3,data_batch4,data_batch5]\n",
        "        # Grabs a list of all the test batches (really just one batch)\n",
        "        self.test_batch = [test_batch]\n",
        "        \n",
        "        # Intialize some empty variables for later on\n",
        "        self.training_images = None\n",
        "        self.training_labels = None\n",
        "        \n",
        "        self.test_images = None\n",
        "        self.test_labels = None\n",
        "    \n",
        "    def set_up_images(self):\n",
        "        \n",
        "        print(\"Setting Up Training Images and Labels\")\n",
        "        \n",
        "        # Vertically stacks the training images\n",
        "        self.training_images = np.vstack([d[b\"data\"] for d in self.all_train_batches])\n",
        "        train_len = len(self.training_images)\n",
        "        \n",
        "        # Reshapes and normalizes training images\n",
        "        self.training_images = self.training_images.reshape(train_len,3,32,32).transpose(0,2,3,1)/255\n",
        "        # One hot Encodes the training labels (e.g. [0,0,0,1,0,0,0,0,0,0])\n",
        "        self.training_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.all_train_batches]), 10)\n",
        "        \n",
        "        print(\"Setting Up Test Images and Labels\")\n",
        "        \n",
        "        # Vertically stacks the test images\n",
        "        self.test_images = np.vstack([d[b\"data\"] for d in self.test_batch])\n",
        "        test_len = len(self.test_images)\n",
        "        \n",
        "        # Reshapes and normalizes test images\n",
        "        self.test_images = self.test_images.reshape(test_len,3,32,32).transpose(0,2,3,1)/255\n",
        "        # One hot Encodes the test labels (e.g. [0,0,0,1,0,0,0,0,0,0])\n",
        "        self.test_labels = one_hot_encode(np.hstack([d[b\"labels\"] for d in self.test_batch]), 10)\n",
        "\n",
        "        \n",
        "    def next_batch(self, batch_size):\n",
        "        # Note that the 100 dimension in the reshape call is set by an assumed batch size of 100\n",
        "        x = self.training_images[self.i:self.i+batch_size].reshape(100,32,32,3)\n",
        "        y = self.training_labels[self.i:self.i+batch_size]\n",
        "        self.i = (self.i + batch_size) % len(self.training_images)\n",
        "        return x, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTukmX9iJJMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "74e0483e-ea35-466a-ab5a-504805fbaaf4"
      },
      "source": [
        "ch = CifarHelper()\n",
        "ch.set_up_images()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Up Training Images and Labels\n",
            "Setting Up Test Images and Labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcJnUrigJJMv",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MKmNkgjJJMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m2lpp0HJJMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.disable_eager_execution()\n",
        "tf.disable_eager_execution()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm-M5M_6JJM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3] )\n",
        "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dXY2nlTJJM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hold_prob = tf.placeholder(tf.float32)\n",
        "hold_prob_1 = tf.placeholder(tf.float32)\n",
        "hold_prob_2 = tf.placeholder(tf.float32)\n",
        "hold_prob_3 = tf.placeholder(tf.float32)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnAWr16hJJM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(shape):\n",
        "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(init_random_dist)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIVWY51tJJNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_bias(shape):\n",
        "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(init_bias_vals)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QBn-oIFJJNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD-OXQl-JJNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_pool_2by2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHxExoiHJJNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_layer(input_x, shape):\n",
        "    W = init_weights(shape)\n",
        "    b = init_bias([shape[3]])\n",
        "    return tf.nn.relu(conv2d(input_x, W) + b)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mhPSJ1vJJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_full_layer(input_layer, size):\n",
        "    input_size = int(input_layer.get_shape()[1])\n",
        "    W = init_weights([input_size, size])\n",
        "    b = init_bias([size])\n",
        "    return tf.matmul(input_layer, W) + b"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_LR42zJJNU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e96404df-1828-43bf-8703-982f608bc972"
      },
      "source": [
        "convo_1 = convolutional_layer(x,shape=[3,3,3,32])\n",
        "convo_1_pooling = max_pool_2by2(convo_1)\n",
        "dropout_layer_1 = tf.nn.dropout(convo_1_pooling, hold_prob_1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-24-38b315ba1595>:3: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz8k4OLgJJNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convo_2 = convolutional_layer(dropout_layer_1, shape=[3,3,32,64])\n",
        "convo_2_pooling = max_pool_2by2(convo_2)\n",
        "dropout_layer_2 = tf.nn.dropout(convo_2_pooling, hold_prob_2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SzPq5IY6pPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convo_3 = convolutional_layer(dropout_layer_2, shape=[3,3,64,128])\n",
        "convo_3_pooling = max_pool_2by2(convo_3)\n",
        "dropout_layer_3 = tf.nn.dropout(convo_3_pooling, hold_prob_3)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVcLgj24JJNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flat_layer = tf.reshape(dropout_layer_3, shape=[-1, 4*4*128])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCBwwGrrJJNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_layer = normal_full_layer(flat_layer, 512)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NutJ41gJJNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_layer = tf.nn.dropout(full_layer, hold_prob)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqbMCKrsJJNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = normal_full_layer(dropout_layer, 10)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQG9MP5pJJNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2c1a2b9e-b2b8-43bf-9eb1-bbd16baf2e6e"
      },
      "source": [
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-31-e0c07892321c>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdxK_kUoJJN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "train = optimizer.minimize(cross_entropy)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h-CPG_YJJN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnofRzphJJN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61402a5e-f965-4f0d-c547-90448159ca74"
      },
      "source": [
        "steps = 200000\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(steps):\n",
        "        \n",
        "        batch_x , batch_y = ch.next_batch(100)\n",
        "        \n",
        "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5, hold_prob_1:0.8,hold_prob_2:0.7,hold_prob_3:0.6})\n",
        "        \n",
        "        # PRINT OUT A MESSAGE EVERY 10000 STEPS\n",
        "        if i%10000 == 0:\n",
        "            \n",
        "            print('Currently on step {}'.format(i))\n",
        "            print('Accuracy is:')\n",
        "            # Test the Train Model\n",
        "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
        "\n",
        "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
        "\n",
        "            print(sess.run(acc,feed_dict={x: ch.test_images,y_true: ch.test_labels,hold_prob:1.0, hold_prob_1:0.8,hold_prob_2:0.7,hold_prob_3:0.6}))\n",
        "            print('\\n')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently on step 0\n",
            "Accuracy is:\n",
            "0.1024\n",
            "\n",
            "\n",
            "Currently on step 10000\n",
            "Accuracy is:\n",
            "0.5532\n",
            "\n",
            "\n",
            "Currently on step 20000\n",
            "Accuracy is:\n",
            "0.6437\n",
            "\n",
            "\n",
            "Currently on step 30000\n",
            "Accuracy is:\n",
            "0.6817\n",
            "\n",
            "\n",
            "Currently on step 40000\n",
            "Accuracy is:\n",
            "0.7043\n",
            "\n",
            "\n",
            "Currently on step 50000\n",
            "Accuracy is:\n",
            "0.7137\n",
            "\n",
            "\n",
            "Currently on step 60000\n",
            "Accuracy is:\n",
            "0.726\n",
            "\n",
            "\n",
            "Currently on step 70000\n",
            "Accuracy is:\n",
            "0.7321\n",
            "\n",
            "\n",
            "Currently on step 80000\n",
            "Accuracy is:\n",
            "0.7398\n",
            "\n",
            "\n",
            "Currently on step 90000\n",
            "Accuracy is:\n",
            "0.7453\n",
            "\n",
            "\n",
            "Currently on step 100000\n",
            "Accuracy is:\n",
            "0.7481\n",
            "\n",
            "\n",
            "Currently on step 110000\n",
            "Accuracy is:\n",
            "0.7496\n",
            "\n",
            "\n",
            "Currently on step 120000\n",
            "Accuracy is:\n",
            "0.7524\n",
            "\n",
            "\n",
            "Currently on step 130000\n",
            "Accuracy is:\n",
            "0.7541\n",
            "\n",
            "\n",
            "Currently on step 140000\n",
            "Accuracy is:\n",
            "0.7468\n",
            "\n",
            "\n",
            "Currently on step 150000\n",
            "Accuracy is:\n",
            "0.7573\n",
            "\n",
            "\n",
            "Currently on step 160000\n",
            "Accuracy is:\n",
            "0.7521\n",
            "\n",
            "\n",
            "Currently on step 170000\n",
            "Accuracy is:\n",
            "0.7537\n",
            "\n",
            "\n",
            "Currently on step 180000\n",
            "Accuracy is:\n",
            "0.7577\n",
            "\n",
            "\n",
            "Currently on step 190000\n",
            "Accuracy is:\n",
            "0.7589\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy7gAnBSJJOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}